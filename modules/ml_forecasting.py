from autogluon.tabular import TabularPredictor
import os
import torch

MODEL_PATH = "/mnt/efs1/ml_models/autogluon_model"

# 游늷 **Detecci칩n autom치tica de GPU o CPU**
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 游늷 **Ajuste Autom치tico del Tama침o de Lote**
def get_optimal_batch_size(base_size=32):
    """Ajusta autom치ticamente el tama침o de lote seg칰n la memoria disponible."""
    try:
        if torch.cuda.is_available():
            total_mem = torch.cuda.get_device_properties(0).total_memory
            return min(512, max(base_size, int(total_mem / 1024**3 * 64)))
        return base_size  # Si no hay GPU, usa el tama침o base
    except:
        return base_size

batch_size = get_optimal_batch_size()

# 游늷 **Ajuste Autom치tico del N칰mero de 칄pocas**
epochs = 3 if torch.cuda.is_available() else 1

# 游늷 **Automatizaci칩n del `learning_rate` basado en la cantidad de datos**
def get_adaptive_learning_rate(base_lr=0.001, scale_factor=0.1):
    """Calcula un learning rate adaptativo en funci칩n de la capacidad de hardware."""
    return base_lr * (scale_factor if torch.cuda.is_available() else scale_factor * 0.5)

learning_rate = get_adaptive_learning_rate()

# Cargar modelo autom치ticamente al iniciar
if os.path.exists(MODEL_PATH):
    predictor = TabularPredictor.load(MODEL_PATH)
else:
    predictor = None

def predict_future_values(data):
    """Realiza una predicci칩n basada en los datos de entrada."""
    if predictor:
        return predictor.predict(data)
    return {"error": "Modelo no disponible."}

def train_model(data, label_column):
    """Entrena o reentrena el modelo con nuevos datos y guarda su progreso."""
    global predictor
    predictor = TabularPredictor(label=label_column).fit(data)
    predictor.save(MODEL_PATH)
    return {"status": "Modelo actualizado y guardado correctamente."}
