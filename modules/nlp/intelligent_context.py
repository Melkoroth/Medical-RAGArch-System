from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
import os
import torch

MODEL_PATH = "/mnt/efs1/nlp_models/transformers_model"

# 游늷 **Detecci칩n autom치tica de GPU o CPU**
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 游늷 **Ajuste Autom치tico del Tama침o de Lote**
def get_optimal_batch_size(base_size=32):
    """Ajusta autom치ticamente el tama침o de lote seg칰n la memoria disponible."""
    try:
        if torch.cuda.is_available():
            total_mem = torch.cuda.get_device_properties(0).total_memory
            return min(512, max(base_size, int(total_mem / 1024**3 * 64)))
        return base_size  # Si no hay GPU, usa el tama침o base
    except:
        return base_size

batch_size = get_optimal_batch_size()

# 游늷 **Ajuste Autom치tico del N칰mero de 칄pocas**
epochs = 3 if torch.cuda.is_available() else 1

# 游늷 **Automatizaci칩n del `learning_rate` con adaptaci칩n al hardware**
def get_adaptive_learning_rate(base_lr=2e-5, scale_factor=0.1):
    """Ajusta el learning rate din치micamente seg칰n la capacidad del hardware."""
    return base_lr * (scale_factor if torch.cuda.is_available() else scale_factor * 0.5)

learning_rate = get_adaptive_learning_rate()

# Cargar modelo autom치ticamente al iniciar
if os.path.exists(MODEL_PATH):
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)
    nlp_pipeline = pipeline("text-classification", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
else:
    tokenizer, model, nlp_pipeline = None, None, None

def analyze_text(text):
    """Analiza el texto utilizando el modelo NLP y devuelve la clasificaci칩n."""
    if nlp_pipeline:
        return nlp_pipeline(text)
    return {"error": "Modelo NLP no disponible."}

def fine_tune_nlp(train_dataset, eval_dataset):
    """Realiza fine-tuning del modelo NLP con los nuevos datos y lo guarda."""
    global model, tokenizer, nlp_pipeline
    if model and tokenizer:
        training_args = TrainingArguments(
            output_dir=MODEL_PATH,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            learning_rate=learning_rate,  # Automatizaci칩n aplicada
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=epochs,
            weight_decay=0.01,
            save_total_limit=1,
            logging_dir="./logs",
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
        )

        trainer.train()
        trainer.save_model(MODEL_PATH)
        tokenizer.save_pretrained(MODEL_PATH)
        
        # 游늷 **Recargar el modelo actualizado despu칠s del fine-tuning**
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)
        nlp_pipeline = pipeline("text-classification", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

        return {"status": "Modelo NLP actualizado y guardado correctamente."}
    return {"error": "No se pudo realizar fine-tuning del modelo NLP."}
